---
title: "Predator Problem"
author: "lyzs90"
date: "2 January 2016"
output: html_document
---

###Introduction

In the past three months, there has been 19 documented incidents of cat abuse in Yishun. Most of the cats were found dead. Even after the arrest of a suspect on 27 Dec, there was yet another incident on 30 Dec. So I've decided to do some rudimentary geographic profiling in **R**. My goal was to see if more than one culprit has been preying on the cats all along or if there was an emergence of a copycat killer. Hopefully, I want to also identify the probable area of residence of the culprit(s). 

###Data Preparation

Referencing [this](http://news.asiaone.com/sites/default/files/cat5.jpg) map done up by the Straits Times, I sourced for the geographic coordinates of all the incidents on Google Maps and keyed the data onto a spreadsheet. Basically, the features I extracted are longitude and latitude, both in decimal degrees. I then normalized the features.

Here is a snapshot of the features used:

```{r, echo=F, message=F, warning=F}
library(dplyr)
library(lubridate)
cat <- read.csv("cat.csv", header=T)
cat$date <- as.numeric(dmy(cat$date)) # convert to POSIX time
cat$norm_lon <- scale(cat$longitude)
cat$norm_lat <- scale(cat$latitude)
cat$norm_date <- scale(cat$date)
df <- cat %>%
        select(norm_lat, norm_lon)
head(df)
```

### How many culprits are there?

Looking at the literature on geographic profiling, there are two accepted beliefs about typical serial criminal behavior:

1. They do not commit crimes too close to their base of operations i.e. the concept of a buffer zone
2. They do not travel further than necessary to find victims

However, it would seem that the first axiom doesn't quite hold as there were two incidents at Block 115B, which is the residence of the arrested suspect. This is of course, assuming that these crimes were committed by the arrested suspect. Which leaves us with the second axiom, that ultimately, the location has to be convenient.

Given the incidents probably took place late at night, so if the culprit walks, the distance he can cover from his home is going to be limited. Unless he cycles, then it's quite possible to cover the entire area if his base is somewhere central. Hence, it seems quite unlikely that the arrested suspect will travel to say Block 355, for instance. This supports the existence of multiple culprits. So for a start, I'm going to use an off-the-shelf k-means algorithm to look for clusters in the data. The algorithm seeks to partition the 19 crime locations into k clusters, whereby each crime location belongs to the cluster with the nearest center of mass. Each cluster could possibly be the area of operations of one culprit with his base being near the center of mass.

First, I determine optimal number of clusters to search for using the NbClust package. This passes the features through 30 indices. Each index will suggest a cluster and then by taking a majority vote, an optimal number of clusters is recommended. 

```{r, echo=F, message=F, warning=F, results='hide', fig.keep='last'}
library(NbClust)
set.seed(12345)
nc <- NbClust(df, min.nc=2, max.nc=5, method="kmeans")
par(mfrow=c(1,1)) # reset plotting
barplot(table(nc$Best.n[1,]),
        xlab="Numer of Clusters", ylab="Number of Indices",
        main="Number of Clusters Chosen by 30 Indices")
```

Given a range of 2-5 clusters to work with, NbClust recommends that we use 3 clusters. Does this mean that there may potentially be up to 3 culprits? 

### Where are the potential bases?

The resultant clusters are shown below, denoted by the crosshairs. The clusters are even sized (about 6-7 incidents each).

```{r, echo=F, message=F, warning=F, results='hide'}
set.seed(12345)
fit.km <- kmeans(df, 3, nstart=25) # do 25 different starts and choose the best
cat$labels <- as.factor(fit.km$cluster)

## obtain unscaled clust means
clust.means <- aggregate(cat[2:3], by=list(labels=fit.km$cluster), mean)
clust.means$labels <- as.factor(clust.means$labels)

## plot on map
library(ggmap)
library(ggplot2)
library(deldir)
library(scales)
# create voronoi line segments using centroids from k-means
xrng <- expand_range(range(clust.means[,3]), .4) # adjust to lengthen voronoi lines
yrng <- expand_range(range(clust.means[,2]), .4) # adjust to lengthen voronoi lines
voronoi <- deldir(x=clust.means[,3], y=clust.means[,2], rw = c(xrng, yrng))
# draw boundary box
lat <- c(1.405, 1.45)                
lon <- c(103.835, 103.84)
map.loc <- get_map(location = c(lon = mean(lon), lat = mean(lat)),
                maptype = "terrain",
                source = "google",
                color = "bw",
                zoom=15)
ggmap(map.loc, extent = 'device')+
    geom_point(data = cat, aes(x = longitude, y = latitude, color = labels), size=4)+
    geom_point(data = clust.means, aes(x = longitude, y = latitude, color=labels),
               size=5,
               shape=10)+
    # plot the voronoi lines
    geom_segment(
    aes(x = x1, y = y1, xend = x2, yend = y2),
    size = 1,
    data = voronoi$dirsgs,
    linetype = 1,
    color= "#FFB958") +
    # plot the annotations
    geom_point(data = cat, aes(x = 103.82688, y = 1.433828), color = "red", size=6, shape="O")+
    geom_point(data = cat, aes(x = 103.830705, y = 1.429337), color = "blue", size=6, shape="O")+
    scale_colour_brewer("Cluster", palette = "Dark2")
```

The purple cluster center is close to the first suspects home (red outline). However, I also noted that the algorithm incorrectly assigned the latest incident on 30 Dec (blue outline) to the purple cluster. Perhaps including the incident date in our feature set could prevent such erroneous assignment? I do have the date variable, but don't quite know how to use it yet. I tried converting it to POSIX time and then normalizing it but that does not seem to produce sensible clusters. I'm thinking of making use of it when normalizing longitude and latitude. Pointers anyone?

### Next Steps

As you can see, the analysis requires more tweaking. The good news is that we have one labelled data point to serve as a cross-check. For those who are interested, the csv and code can be found on my GitHub [repo](https://github.com/lyzs90/PredatorProblem). Do let me know if you have any feedback or suggestions. More data would be greatly appreciated also. Meanwhile to all Yishunites, please help keep a lookout for our feline friends.